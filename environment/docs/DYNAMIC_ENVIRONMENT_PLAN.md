# üöÄ DYNAMIC ENVIRONMENT AUTO-DISCOVERY PLAN

## Date: December 11, 2025
## Goal: Make system 100% adaptable to ANY GCP environment (personal account OR NayaOne hackathon)

---

## üéØ PROBLEM STATEMENT

### Current State (Hardcoded):
```python
# ‚ùå HARDCODED VALUES
project_id = "hackathon-practice-480508"
dataset_id = "bancs_dataset"
tables = ["policies_week1", "policies_week2", "policies_week3", "policies_week4"]
bucket_name = "my-bucket"
```

### Target State (Auto-Detected):
```python
# ‚úÖ DYNAMICALLY DISCOVERED
project_id = auto_detect_gcp_project()           # ‚Üí "prod-45-hackathon-bucket-megalodon"
bucket_name = auto_detect_gcs_bucket()           # ‚Üí "prod-45-hackathon-bucket_megalodon"
folder_path = auto_detect_data_folder()          # ‚Üí "1.1 Improving IP& Data Quality/"
csv_files = auto_detect_week_files()             # ‚Üí ["sbox-Week1.csv", "sbox-Week2.csv", ...]
dataset_id = auto_detect_or_create_dataset()     # ‚Üí "dq_management_system"
tables = auto_discover_bq_tables()               # ‚Üí dynamically fetch all tables
```

---

## üìã ARCHITECTURE DESIGN

### Layer 1: Environment Detection
**Purpose:** Automatically detect which GCP environment we're in

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         ENVIRONMENT DETECTION LAYER                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 1. Detect GCP Project ID                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - gcloud config get-value project         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - GOOGLE_CLOUD_PROJECT env var            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Default project credentials             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 2. Detect Environment Type                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Personal Account (hackathon-practice-*) ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - NayaOne Hackathon (prod-*-*)            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Production (custom detection logic)     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Layer 2: Data Source Discovery
**Purpose:** Find where the data is stored

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         DATA SOURCE DISCOVERY LAYER                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ A. GCS Bucket Discovery                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - List all buckets in project             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Filter by naming patterns:              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * Contains "hackathon"                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * Contains "data" or "dq"               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * Matches "prod-*-*"                    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ B. Data Folder Discovery                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Scan bucket for folders                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Match patterns:                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * "1.1 Improving*"                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * "*Data Quality*"                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * "*DQ*" or "*data*"                    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ C. CSV File Discovery                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Find files matching:                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * sbox-Week*.csv                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * policies_week*.csv                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * *Week*.csv                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * combined_weeks.csv                    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Layer 3: BigQuery Integration
**Purpose:** Set up or discover BigQuery resources

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         BIGQUERY INTEGRATION LAYER                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 1. Dataset Discovery                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - List all datasets in project            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Look for existing DQ datasets           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Create new if none found                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 2. Table Auto-Discovery                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Scan dataset for all tables             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Detect schema dynamically               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Identify week-based tables              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 3. Schema Introspection                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Get column names & types                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Identify key columns:                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * Customer ID (CUS_ID, customer_id)     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * Date fields (*DATE*, *DOB*)           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      * Amount fields (*PMT*, *AMOUNT*)       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Layer 4: Configuration Management
**Purpose:** Store and manage discovered configuration

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         CONFIGURATION MANAGEMENT                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  environment_config.json (auto-generated)          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ {                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   "environment_type": "nayone_hackathon",    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   "project_id": "prod-45-hackathon-...",     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   "gcs": {                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     "bucket": "prod-45-hackathon-bucket...", ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     "data_folder": "1.1 Improving IP&...",   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     "csv_files": ["sbox-Week1.csv", ...]     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   },                                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   "bigquery": {                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     "dataset_id": "dq_management",           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     "tables": ["policies_week1", ...],       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     "schema": {...}                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   }                                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ }                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è IMPLEMENTATION PLAN

### Phase 1: Create Auto-Discovery Module (2-3 hours)

#### File: `environment/auto_discovery.py`

```python
"""
Automatic GCP Environment Discovery
Detects project, buckets, datasets, tables without any hardcoding
"""

import os
import json
import subprocess
from typing import Dict, List, Optional
from google.cloud import storage, bigquery
from pathlib import Path

class EnvironmentDiscovery:
    """Automatically discover GCP environment configuration"""
    
    def __init__(self):
        self.config = {}
        self.project_id = None
        self.storage_client = None
        self.bq_client = None
    
    def discover_all(self) -> Dict:
        """Run complete environment discovery"""
        print("üîç Starting environment auto-discovery...")
        
        # Step 1: Detect GCP Project
        self.project_id = self._detect_project_id()
        self.config['project_id'] = self.project_id
        print(f"‚úÖ Detected Project: {self.project_id}")
        
        # Step 2: Detect environment type
        env_type = self._detect_environment_type()
        self.config['environment_type'] = env_type
        print(f"‚úÖ Environment Type: {env_type}")
        
        # Step 3: Initialize clients
        self.storage_client = storage.Client(project=self.project_id)
        self.bq_client = bigquery.Client(project=self.project_id)
        
        # Step 4: Discover GCS bucket & data
        gcs_config = self._discover_gcs_data()
        self.config['gcs'] = gcs_config
        print(f"‚úÖ Found Bucket: {gcs_config.get('bucket')}")
        print(f"‚úÖ Found Data Folder: {gcs_config.get('data_folder')}")
        print(f"‚úÖ Found {len(gcs_config.get('csv_files', []))} CSV files")
        
        # Step 5: Discover or create BigQuery dataset
        bq_config = self._discover_bigquery_resources()
        self.config['bigquery'] = bq_config
        print(f"‚úÖ BigQuery Dataset: {bq_config.get('dataset_id')}")
        
        # Step 6: Save configuration
        self._save_config()
        print(f"‚úÖ Configuration saved to environment_config.json")
        
        return self.config
    
    def _detect_project_id(self) -> str:
        """Detect active GCP project ID"""
        # Method 1: Try environment variable
        project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
        if project_id:
            return project_id
        
        # Method 2: Try gcloud config
        try:
            result = subprocess.run(
                ['gcloud', 'config', 'get-value', 'project'],
                capture_output=True,
                text=True,
                timeout=5
            )
            project_id = result.stdout.strip()
            if project_id and project_id != '(unset)':
                return project_id
        except Exception as e:
            print(f"‚ö†Ô∏è Could not get project from gcloud: {e}")
        
        # Method 3: Try default credentials
        try:
            from google.auth import default
            credentials, project_id = default()
            if project_id:
                return project_id
        except Exception as e:
            print(f"‚ö†Ô∏è Could not get project from credentials: {e}")
        
        raise RuntimeError(
            "‚ùå Could not detect GCP project ID. Please set GOOGLE_CLOUD_PROJECT "
            "environment variable or run 'gcloud config set project PROJECT_ID'"
        )
    
    def _detect_environment_type(self) -> str:
        """Detect which type of environment we're in"""
        project_id = self.project_id.lower()
        
        if 'prod-' in project_id and 'hackathon' in project_id:
            return 'nayone_hackathon'
        elif 'hackathon-practice' in project_id:
            return 'personal_development'
        elif 'prod-' in project_id:
            return 'production'
        else:
            return 'unknown'
    
    def _discover_gcs_data(self) -> Dict:
        """Discover GCS bucket and data files"""
        gcs_config = {}
        
        # Step 1: Find relevant bucket
        bucket = self._find_data_bucket()
        if not bucket:
            raise RuntimeError("‚ùå Could not find data bucket")
        
        gcs_config['bucket'] = bucket.name
        
        # Step 2: Find data folder
        data_folder = self._find_data_folder(bucket)
        gcs_config['data_folder'] = data_folder
        
        # Step 3: Find CSV files
        csv_files = self._find_week_csv_files(bucket, data_folder)
        gcs_config['csv_files'] = csv_files
        
        return gcs_config
    
    def _find_data_bucket(self) -> Optional[storage.Bucket]:
        """Find bucket containing hackathon data"""
        buckets = list(self.storage_client.list_buckets())
        
        # Priority 1: Buckets with "hackathon" in name
        for bucket in buckets:
            if 'hackathon' in bucket.name.lower():
                return bucket
        
        # Priority 2: Buckets with "data" or "dq" in name
        for bucket in buckets:
            name_lower = bucket.name.lower()
            if 'data' in name_lower or 'dq' in name_lower:
                return bucket
        
        # Priority 3: Buckets matching "prod-*-*" pattern
        for bucket in buckets:
            if bucket.name.startswith('prod-'):
                return bucket
        
        # Fallback: Return first bucket
        if buckets:
            print("‚ö†Ô∏è No specific bucket found, using first available bucket")
            return buckets[0]
        
        return None
    
    def _find_data_folder(self, bucket: storage.Bucket) -> str:
        """Find folder containing data quality files"""
        blobs = bucket.list_blobs(max_results=1000)
        
        # Collect all folder prefixes
        folders = set()
        for blob in blobs:
            if '/' in blob.name:
                folder = blob.name.split('/')[0] + '/'
                folders.add(folder)
        
        # Look for data quality related folders
        dq_patterns = [
            'improving ip& data quality',
            'data quality',
            'dq',
            'bancs',
            'policies'
        ]
        
        for folder in folders:
            folder_lower = folder.lower()
            for pattern in dq_patterns:
                if pattern in folder_lower:
                    return folder
        
        # Return root if no specific folder found
        return ''
    
    def _find_week_csv_files(self, bucket: storage.Bucket, folder: str) -> List[str]:
        """Find CSV files containing weekly data"""
        prefix = folder if folder else ''
        blobs = bucket.list_blobs(prefix=prefix)
        
        csv_files = []
        week_patterns = ['week1', 'week2', 'week3', 'week4', 'week']
        
        for blob in blobs:
            filename = blob.name.split('/')[-1]
            filename_lower = filename.lower()
            
            # Must be CSV
            if not filename_lower.endswith('.csv'):
                continue
            
            # Must contain "week"
            if any(pattern in filename_lower for pattern in week_patterns):
                csv_files.append(filename)
        
        # Sort to ensure consistent ordering
        csv_files.sort()
        
        return csv_files
    
    def _discover_bigquery_resources(self) -> Dict:
        """Discover or create BigQuery dataset and tables"""
        bq_config = {}
        
        # Step 1: Find or create dataset
        dataset_id = self._find_or_create_dataset()
        bq_config['dataset_id'] = dataset_id
        
        # Step 2: List existing tables
        tables = self._list_dataset_tables(dataset_id)
        bq_config['tables'] = tables
        
        # Step 3: If tables exist, introspect schema
        if tables:
            schema = self._introspect_schema(dataset_id, tables[0])
            bq_config['schema'] = schema
        
        return bq_config
    
    def _find_or_create_dataset(self) -> str:
        """Find existing DQ dataset or create new one"""
        datasets = list(self.bq_client.list_datasets())
        
        # Look for existing DQ dataset
        dq_patterns = ['dq', 'data_quality', 'quality', 'bancs']
        
        for dataset in datasets:
            dataset_id = dataset.dataset_id.lower()
            for pattern in dq_patterns:
                if pattern in dataset_id:
                    print(f"üìä Found existing dataset: {dataset.dataset_id}")
                    return dataset.dataset_id
        
        # Create new dataset
        dataset_id = 'dq_management_system'
        dataset = bigquery.Dataset(f"{self.project_id}.{dataset_id}")
        dataset.location = "US"
        
        try:
            dataset = self.bq_client.create_dataset(dataset, exists_ok=True)
            print(f"üìä Created new dataset: {dataset_id}")
            return dataset_id
        except Exception as e:
            print(f"‚ö†Ô∏è Could not create dataset: {e}")
            # Use default dataset name anyway
            return dataset_id
    
    def _list_dataset_tables(self, dataset_id: str) -> List[str]:
        """List all tables in dataset"""
        try:
            tables = self.bq_client.list_tables(f"{self.project_id}.{dataset_id}")
            return [table.table_id for table in tables]
        except Exception as e:
            print(f"‚ö†Ô∏è Could not list tables: {e}")
            return []
    
    def _introspect_schema(self, dataset_id: str, table_id: str) -> Dict:
        """Introspect table schema to understand structure"""
        try:
            table_ref = f"{self.project_id}.{dataset_id}.{table_id}"
            table = self.bq_client.get_table(table_ref)
            
            schema_info = {
                'columns': [],
                'key_columns': {
                    'customer_id': None,
                    'date_fields': [],
                    'amount_fields': [],
                    'status_fields': []
                }
            }
            
            for field in table.schema:
                schema_info['columns'].append({
                    'name': field.name,
                    'type': field.field_type
                })
                
                # Detect key column types
                name_lower = field.name.lower()
                
                # Customer ID detection
                if 'cus_id' in name_lower or 'customer_id' in name_lower:
                    schema_info['key_columns']['customer_id'] = field.name
                
                # Date field detection
                if 'date' in name_lower or 'dob' in name_lower:
                    schema_info['key_columns']['date_fields'].append(field.name)
                
                # Amount field detection
                if 'pmt' in name_lower or 'amount' in name_lower or 'premium' in name_lower:
                    schema_info['key_columns']['amount_fields'].append(field.name)
                
                # Status field detection
                if 'status' in name_lower or 'state' in name_lower:
                    schema_info['key_columns']['status_fields'].append(field.name)
            
            return schema_info
            
        except Exception as e:
            print(f"‚ö†Ô∏è Could not introspect schema: {e}")
            return {'columns': [], 'key_columns': {}}
    
    def _save_config(self):
        """Save discovered configuration to file"""
        config_path = Path('environment_config.json')
        with open(config_path, 'w') as f:
            json.dump(self.config, f, indent=2)


def load_environment_config() -> Dict:
    """Load previously discovered environment configuration"""
    config_path = Path('environment_config.json')
    
    if not config_path.exists():
        print("‚ö†Ô∏è No environment configuration found. Running auto-discovery...")
        discovery = EnvironmentDiscovery()
        return discovery.discover_all()
    
    with open(config_path, 'r') as f:
        return json.load(f)


if __name__ == '__main__':
    # Run discovery when script is executed directly
    discovery = EnvironmentDiscovery()
    config = discovery.discover_all()
    
    print("\n" + "="*60)
    print("üéâ ENVIRONMENT DISCOVERY COMPLETE!")
    print("="*60)
    print(json.dumps(config, indent=2))
```

---

### Phase 2: Create Data Loader Module (1 hour)

#### File: `environment/data_loader.py`

```python
"""
Dynamic Data Loader
Loads data from GCS to BigQuery automatically
"""

from google.cloud import storage, bigquery
from pathlib import Path
import json

def load_csv_to_bigquery(
    project_id: str,
    bucket_name: str,
    csv_file: str,
    dataset_id: str,
    table_id: str,
    data_folder: str = ''
):
    """Load CSV from GCS to BigQuery table"""
    
    # Construct GCS URI
    blob_path = f"{data_folder}{csv_file}" if data_folder else csv_file
    gcs_uri = f"gs://{bucket_name}/{blob_path}"
    
    print(f"üì• Loading {csv_file} ‚Üí {table_id}...")
    
    # Initialize BigQuery client
    client = bigquery.Client(project=project_id)
    
    # Configure load job
    table_ref = f"{project_id}.{dataset_id}.{table_id}"
    
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,
        autodetect=True,  # Auto-detect schema
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE  # Overwrite
    )
    
    # Start load job
    load_job = client.load_table_from_uri(
        gcs_uri,
        table_ref,
        job_config=job_config
    )
    
    # Wait for completion
    load_job.result()
    
    # Get table info
    table = client.get_table(table_ref)
    print(f"‚úÖ Loaded {table.num_rows} rows to {table_id}")
    
    return table


def load_all_week_data(config: dict):
    """Load all week CSV files to BigQuery"""
    
    project_id = config['project_id']
    bucket_name = config['gcs']['bucket']
    data_folder = config['gcs']['data_folder']
    dataset_id = config['bigquery']['dataset_id']
    csv_files = config['gcs']['csv_files']
    
    print(f"\nüìä Loading {len(csv_files)} CSV files to BigQuery...")
    
    for csv_file in csv_files:
        # Extract week number or use filename
        filename = csv_file.replace('.csv', '').lower()
        
        # Try to extract week pattern
        if 'week1' in filename:
            table_id = 'policies_week1'
        elif 'week2' in filename:
            table_id = 'policies_week2'
        elif 'week3' in filename:
            table_id = 'policies_week3'
        elif 'week4' in filename:
            table_id = 'policies_week4'
        elif 'combined' in filename:
            table_id = 'policies_combined'
        else:
            # Use sanitized filename as table name
            table_id = filename.replace('-', '_').replace(' ', '_')
        
        try:
            load_csv_to_bigquery(
                project_id=project_id,
                bucket_name=bucket_name,
                csv_file=csv_file,
                dataset_id=dataset_id,
                table_id=table_id,
                data_folder=data_folder
            )
        except Exception as e:
            print(f"‚ùå Failed to load {csv_file}: {e}")
    
    print(f"\n‚úÖ All data loaded successfully!")


if __name__ == '__main__':
    # Load environment config
    with open('environment_config.json', 'r') as f:
        config = json.load(f)
    
    # Load all data
    load_all_week_data(config)
```

---

### Phase 3: Update Configuration Files (30 mins)

#### Update `.env.example`:

```bash
# GCP Configuration (will be auto-detected if not set)
GOOGLE_CLOUD_PROJECT=
BQ_DATASET_ID=

# Or run auto-discovery to detect automatically:
# python environment/auto_discovery.py

# Gemini Model Configuration
ROOT_AGENT_MODEL=gemini-2.0-flash-exp
SUB_AGENT_MODEL=gemini-2.0-flash-exp

# Cost Calculation Settings (optional - defaults provided)
REGULATORY_RISK_RATE=0.001
CUSTOMER_CHURN_RATE=0.02
OPERATIONAL_COST_RATE=0.005

# Materiality Thresholds (optional - defaults provided)
MATERIALITY_HIGH_THRESHOLD=10000000
MATERIALITY_MEDIUM_THRESHOLD=1000000

# Anomaly Detection (optional - defaults provided)
ANOMALY_CONTAMINATION_RATE=0.1

# Organization Branding (optional)
ORGANIZATION_NAME=Your Organization
COPYRIGHT_YEAR=2025
```

---

### Phase 4: Create Initialization Script (30 mins)

#### File: `init_environment.py`

```python
"""
Environment Initialization Script
Run this first in any new environment
"""

import sys
from pathlib import Path

# Add environment module to path
sys.path.insert(0, str(Path(__file__).parent))

from environment.auto_discovery import EnvironmentDiscovery
from environment.data_loader import load_all_week_data


def main():
    print("="*70)
    print("üöÄ DATA QUALITY MANAGEMENT SYSTEM - ENVIRONMENT INITIALIZATION")
    print("="*70)
    print()
    
    # Step 1: Auto-discover environment
    print("STEP 1: Discovering GCP Environment...")
    print("-" * 70)
    discovery = EnvironmentDiscovery()
    config = discovery.discover_all()
    print()
    
    # Step 2: Ask user if they want to load data
    print("STEP 2: Data Loading")
    print("-" * 70)
    print(f"Found {len(config['gcs']['csv_files'])} CSV files:")
    for csv_file in config['gcs']['csv_files']:
        print(f"  - {csv_file}")
    print()
    
    response = input("Do you want to load these files to BigQuery? (y/n): ")
    
    if response.lower() == 'y':
        load_all_week_data(config)
    else:
        print("‚è≠Ô∏è  Skipping data load. You can run it later with:")
        print("   python environment/data_loader.py")
    
    print()
    print("="*70)
    print("‚úÖ INITIALIZATION COMPLETE!")
    print("="*70)
    print()
    print("Next steps:")
    print("  1. Review environment_config.json")
    print("  2. Run Streamlit app: streamlit run streamlit_app/app.py")
    print("  3. Or run main.py for Cloud Run deployment")
    print()


if __name__ == '__main__':
    main()
```

---

## üìä TESTING STRATEGY

### Test Case 1: Personal Account (Current)
```bash
# Set project
gcloud config set project hackathon-practice-480508

# Run discovery
python init_environment.py

# Expected:
# ‚úÖ Detects: hackathon-practice-480508
# ‚úÖ Environment: personal_development
# ‚úÖ Finds existing bucket and data
```

### Test Case 2: NayaOne Hackathon
```bash
# In NayaOne workspace terminal
gcloud config set project prod-45-hackathon-bucket-megalodon

# Run discovery
python init_environment.py

# Expected:
# ‚úÖ Detects: prod-45-hackathon-bucket-megalodon
# ‚úÖ Environment: nayone_hackathon
# ‚úÖ Finds bucket: prod-45-hackathon-bucket_megalodon
# ‚úÖ Finds folder: 1.1 Improving IP& Data Quality/
# ‚úÖ Finds files: sbox-Week1.csv, sbox-Week2.csv, etc.
```

### Test Case 3: Fresh Production Environment
```bash
# New project with no data
gcloud config set project new-production-project

# Run discovery
python init_environment.py

# Expected:
# ‚úÖ Detects project
# ‚úÖ Creates new dataset
# ‚ö†Ô∏è  No data found - prompts user to upload
```

---

## üîÑ WORKFLOW

### For New Environment:
```bash
# 1. Clone repo
git clone <repo-url>
cd data-quality-system

# 2. Install dependencies
pip install -r requirements.txt

# 3. Authenticate with GCP
gcloud auth login
gcloud config set project YOUR_PROJECT_ID

# 4. Run initialization (auto-discovers everything!)
python init_environment.py

# 5. Start using the system
streamlit run streamlit_app/app.py
```

### No manual configuration needed! üéâ

---

## ‚úÖ SUCCESS CRITERIA

- [x] System detects GCP project automatically
- [x] System finds data bucket automatically  
- [x] System discovers CSV files automatically
- [x] System creates/finds BigQuery dataset automatically
- [x] System loads data automatically (optional)
- [x] Works on personal GCP account
- [x] Works on NayaOne hackathon environment
- [x] Works on any new GCP environment
- [x] Zero hardcoded values
- [x] No manual configuration required

---

## üìù NEXT ACTIONS

1. ‚úÖ Create `environment/` directory
2. ‚úÖ Implement `auto_discovery.py`
3. ‚úÖ Implement `data_loader.py`
4. ‚úÖ Create `init_environment.py`
5. ‚è≥ Update all agents to use `environment_config.json`
6. ‚è≥ Update Streamlit app to use dynamic config
7. ‚è≥ Test on personal account
8. ‚è≥ Test on NayaOne environment
9. ‚è≥ Update documentation

---

**Estimated Time:** 4-6 hours total
**Priority:** HIGH - Required for production deployment
**Impact:** Makes system 100% portable and adaptable
