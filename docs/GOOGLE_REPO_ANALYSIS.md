# Google ADK Data Science Repo Usage Analysis

## Executive Summary

**Current Status:** âš ï¸ **PARTIALLY LEVERAGING** - We're using basic ADK patterns but missing advanced capabilities from the Google repo.

**Recommendation:** ðŸ”„ **ENHANCE** - Integrate key patterns from the Google data science agent for better DQ operations.

---

## What We're Currently Using âœ…

### 1. **ADK Core Framework** (Using Correctly)
- âœ… `LlmAgent` from `google.adk.agents`
- âœ… `Runner` from `google.adk.runners`
- âœ… `InMemorySessionService` and `InMemoryArtifactService`
- âœ… `ToolContext` for tool functions
- âœ… `types.GenerateContentConfig` for temperature control
- âœ… Sub-agent structure (we have `dq_agents/identifier/`)

### 2. **BigQuery Integration** (Basic)
- âœ… Using `google.cloud.bigquery.Client` directly
- âœ… Basic schema retrieval
- âœ… Sample data querying
- âœ… SQL execution

---

## What We're MISSING from Google Repo ðŸš¨

### 1. **ADK BigQuery Tools** (NOT USING)

**Google Repo Has:**
```python
from google.adk.tools.bigquery.client import get_bigquery_client
```

**Benefits We're Missing:**
- Built-in user agent tracking
- Better error handling
- Standardized connection pooling
- ADK-integrated logging

**Current Impact:** Low (our basic client works, but less production-ready)

---

### 2. **Schema + Samples Pattern** (PARTIALLY USING)

**Google Repo Pattern:**
```python
def get_bigquery_schema_and_samples():
    """From data_science/sub_agents/bigquery/tools.py"""
    tables_context = {}
    for table in client.list_tables(dataset_ref):
        table_info = client.get_table(...)
        table_schema = [
            (schema_field.name, schema_field.field_type)
            for schema_field in table_info.schema
        ]
        sample_values = []
        sample_query = f"SELECT * FROM `{table_ref}` LIMIT 5"
        sample_values = (
            client.query(sample_query).to_dataframe().to_dict(orient="list")
        )
        # Serialize values for SQL literals
        for key in sample_values:
            sample_values[key] = [
                _serialize_value_for_sql(v) for v in sample_values[key]
            ]
        tables_context[str(table_ref)] = {
            "table_schema": table_schema,
            "example_values": sample_values,
        }
    return tables_context
```

**What We Have:**
```python
def get_table_schema_with_samples(table_name: str, sample_rows: int = 10):
    # Similar but less robust
    # Missing: _serialize_value_for_sql for proper SQL literal formatting
    # Missing: DataFrame conversion for better data handling
```

**Current Impact:** Medium (our version works but less production-ready)

---

### 3. **Database Settings Pattern** (NOT USING)

**Google Repo Has:**
```python
def get_database_settings():
    """Caches and manages database configuration globally"""
    global database_settings
    if database_settings is None:
        database_settings = update_database_settings()
    return database_settings
```

**Benefits We're Missing:**
- Centralized config management
- Caching to avoid repeated queries
- Consistent schema representation across all tools

**Our Current Approach:**
- Environment variables directly in each tool
- No caching
- Repeated client initialization

**Current Impact:** Medium-High (performance and maintainability issue)

---

### 4. **CallbackContext Pattern** (NOT USING)

**Google Repo Has:**
```python
def load_database_settings_in_context(callback_context: CallbackContext):
    """Load database settings into the callback context on first use."""
    if "database_settings" not in callback_context.state:
        callback_context.state["database_settings"] = _database_settings
```

**Used in Agent:**
```python
agent = LlmAgent(
    before_agent_callback=load_database_settings_in_context,
    # ... other params
)
```

**Benefits We're Missing:**
- State management across tool calls
- Lazy loading of expensive operations
- Better memory management

**Current Impact:** High (efficiency and scalability issue)

---

### 5. **SQL Serialization Utilities** (NOT USING)

**Google Repo Has:**
```python
def _serialize_value_for_sql(value):
    """Serializes Python values into BigQuery SQL literals"""
    if isinstance(value, (list, np.ndarray)):
        return f"[{', '.join(_serialize_value_for_sql(v) for v in value)}]"
    if pd.isna(value):
        return "NULL"
    if isinstance(value, str):
        new_value = value.replace("\\", "\\\\").replace("'", "''")
        return f"'{new_value}'"
    # ... handles bytes, datetime, STRUCT, etc.
```

**Why We Need This:**
When generating SQL DQ rules, we need to properly escape:
- Strings with quotes
- NULL values
- Arrays
- Date/timestamps
- Complex types

**Current Impact:** High (SQL injection risk, broken queries with special chars)

---

### 6. **ChaseSQL Integration** (NOT USING AT ALL)

**Google Repo Has:**
```python
from .chase_sql import chase_constants
from .chase_sql.llm_utils import GeminiModel
from .chase_sql.dc_prompt_template import DC_PROMPT_TEMPLATE
from .chase_sql.qp_prompt_template import QP_PROMPT_TEMPLATE
```

**What It Does:**
- Advanced NL2SQL with "Divide and Conquer" prompting
- Query Plan-based SQL generation
- SQL post-processing and correction
- Specialized prompt templates for complex SQL

**Relevance to DQ Use Case:**
- ðŸ”¥ HIGH - We generate SQL DQ rules from natural language
- Could significantly improve SQL quality
- Handles complex joins for cross-week temporal rules

**Current Impact:** High (missed opportunity for better SQL generation)

---

### 7. **Sub-Agent Pattern** (USING BUT COULD ENHANCE)

**Google Repo Pattern:**
```python
def get_root_agent() -> LlmAgent:
    tools = [call_analytics_agent]
    sub_agents = []
    for dataset in _dataset_config["datasets"]:
        if dataset["type"] == "bigquery":
            tools.append(call_bigquery_agent)
            sub_agents.append(bqml_agent)
        elif dataset["type"] == "alloydb":
            tools.append(call_alloydb_agent)
    
    agent = LlmAgent(
        sub_agents=sub_agents,  # <-- Pass sub-agents here
        tools=tools,
        # ...
    )
```

**Our Current Approach:**
- Have `dq_agents/identifier/` structure
- Not using ADK's `sub_agents` parameter
- Manual orchestration in Streamlit

**Future Benefit:**
- Treatment, Remediator, Metrics agents as sub-agents
- Better orchestration through ADK
- Cleaner agent collaboration

**Current Impact:** Medium (will matter when we add more agents)

---

## Specific Recommendations for DQ System

### Priority 1: HIGH ðŸ”´

#### 1.1 **Add SQL Serialization Utility**
```python
# dq_agents/identifier/tools.py
def _serialize_value_for_sql(value):
    """Copy from google repo - handles SQL escaping properly"""
    # This prevents SQL injection in generated rules
    # Handles NULL, strings, arrays, dates properly
```

**Why:** Our generated DQ rules have SQL injection risk and break on special characters.

#### 1.2 **Implement Database Settings Cache**
```python
# dq_agents/identifier/tools.py
_database_settings = None

def get_database_settings():
    global _database_settings
    if _database_settings is None:
        _database_settings = {
            "project_id": os.getenv("BQ_DATA_PROJECT_ID"),
            "dataset_id": os.getenv("BQ_DATASET_ID"),
            "schema": get_all_schemas_cached(),
        }
    return _database_settings
```

**Why:** We're querying BigQuery metadata repeatedly (expensive, slow).

#### 1.3 **Use ADK BigQuery Client**
```python
from google.adk.tools.bigquery.client import get_bigquery_client
from data_science.utils.utils import USER_AGENT

def get_table_schema(table_name: str, tool_context: ToolContext):
    client = get_bigquery_client(
        project=compute_project,
        credentials=None,
        user_agent=USER_AGENT,
    )
    # ... rest of code
```

**Why:** Better logging, tracking, and production readiness.

---

### Priority 2: MEDIUM ðŸŸ¡

#### 2.1 **Add CallbackContext for State Management**
```python
# dq_agents/identifier/agent.py
def load_dq_settings_in_context(callback_context: CallbackContext):
    if "dq_settings" not in callback_context.state:
        callback_context.state["dq_settings"] = {
            "preexisting_rules": load_preexisting_rules_cached(),
            "week_tables": get_all_week_tables_cached(),
            "dataplex_profiles": {},  # Lazy load
        }

identifier_agent = LlmAgent(
    before_agent_callback=load_dq_settings_in_context,
    # ... other params
)
```

**Why:** Avoid reloading pre-existing rules and week tables on every agent call.

#### 2.2 **Enhance Schema+Samples with DataFrame Pattern**
```python
def get_table_schema_with_samples(table_name: str, sample_rows: int = 10):
    # Get as DataFrame (like Google repo)
    sample_df = client.query(sample_query).to_dataframe()
    
    # Convert to dict with proper serialization
    sample_values = sample_df.to_dict(orient="list")
    for key in sample_values:
        sample_values[key] = [
            _serialize_value_for_sql(v) for v in sample_values[key]
        ]
    
    # Return formatted
    return json.dumps({
        "table_name": table_name,
        "schema": table_schema,
        "example_values": sample_values,  # Now properly escaped
    })
```

**Why:** Better data handling and SQL-safe values for rule generation.

---

### Priority 3: OPTIONAL (Future Enhancement) ðŸŸ¢

#### 3.1 **ChaseSQL Integration for Complex Rules**
- Copy ChaseSQL module from Google repo
- Use for cross-week temporal rules (complex JOINs)
- Apply to natural language â†’ SQL DQ rule generation

**Why:** Would dramatically improve SQL quality for complex temporal rules.

#### 3.2 **Sub-Agent Orchestration via ADK**
- When Treatment/Remediator/Metrics agents are built
- Use ADK's `sub_agents` parameter
- Better than manual Streamlit orchestration

**Why:** Cleaner architecture, better observability with ADK tooling.

---

## Current Code Quality Assessment

### What's Working Well âœ…
1. Basic agent structure is correct
2. Tool functions follow ADK patterns
3. Streamlit integration is functional
4. Natural language to SQL generation works

### What Needs Improvement âš ï¸
1. **SQL Injection Risk:** No value escaping in generated SQL
2. **Performance:** Repeated BigQuery metadata queries (should cache)
3. **Error Handling:** Basic try/except, could use Google repo's patterns
4. **Production Readiness:** Missing user agent tracking, proper logging

### Technical Debt ðŸ”§
1. No centralized database settings management
2. No callback context for state management
3. Missing SQL serialization utilities
4. Not using ADK's BigQuery client helper

---

## Action Plan

### Immediate (Today)
1. âœ… Copy `_serialize_value_for_sql()` from Google repo â†’ `dq_agents/identifier/tools.py`
2. âœ… Implement database settings caching pattern
3. âœ… Switch to ADK BigQuery client

### Short-term (Before Demo)
4. Add callback context pattern for state management
5. Enhance `get_table_schema_with_samples()` with DataFrame pattern
6. Add proper error handling from Google repo patterns

### Optional (Post-Demo)
7. Integrate ChaseSQL for complex temporal rules
8. Migrate to sub-agent pattern when building Treatment/Remediator agents
9. Add observability/logging from Google repo

---

## Conclusion

**Are we using the Google repo?** 
- âœ… Yes for core ADK patterns
- âš ï¸ No for advanced BigQuery tooling
- âŒ No for ChaseSQL (missed opportunity)
- âš ï¸ Partially for schema+samples pattern

**Should we enhance?**
- ðŸ”´ HIGH PRIORITY: SQL serialization (security risk)
- ðŸŸ¡ MEDIUM PRIORITY: Caching and ADK client (performance)
- ðŸŸ¢ OPTIONAL: ChaseSQL integration (quality improvement)

**Overall Assessment:** 
Our Identifier Agent works but is **not production-grade**. Google repo has battle-tested patterns we should adopt, especially for:
1. SQL safety (escaping/injection prevention)
2. Performance (caching)
3. Production readiness (logging, user agent)

**Estimated Enhancement Time:** 2-3 hours to reach production quality.
